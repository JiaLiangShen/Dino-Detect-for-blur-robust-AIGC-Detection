import torch
import torch.nn.functional as F
from PIL import Image
import torchvision.transforms as transforms
import numpy as np
import matplotlib.pyplot as plt
import cv2
from pathlib import Path
import os
from sklearn.decomposition import PCA
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
import threading
import random
from transformers import AutoImageProcessor, AutoModel
import open_clip
import torch.nn as nn
import json # æ–°å¢: ç”¨äºä¿å­˜JSON
import seaborn as sns # æ–°å¢: finalç»˜å›¾ç”¨åˆ°
import timm

MOTION_BLUR_ANALYSIS = True  # æ˜¯å¦å¯ç”¨è¿åŠ¨æ¨¡ç³Šåˆ†æ
BLUR_STRENGTH_RANGE = (0.00, 0.20, 0.025)  # (æœ€å°å€¼, æœ€å¤§å€¼, æ­¥é•¿)
BLUR_ANGLE = 0  # è¿åŠ¨æ¨¡ç³Šè§’åº¦
MOTION_BLUR_OUTPUT_DIR = "attention_similarity_analysis"  # è¿åŠ¨æ¨¡ç³Šåˆ†æè¾“å‡ºæ–‡ä»¶å¤¹

class VisionTransformer(nn.Module):
    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, 
                 depth=12, num_heads=12, mlp_ratio=4.):
        super().__init__()
        self.patch_size = patch_size
        num_patches = (img_size // patch_size) ** 2
        
        self.patch_embed = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)
        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))
        
        self.blocks = nn.ModuleList([
            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, 
                                       dim_feedforward=int(embed_dim * mlp_ratio),
                                       batch_first=True)
            for _ in range(depth)
        ])
        
        self.norm = nn.LayerNorm(embed_dim)
        
    def forward(self, x):
        B = x.shape[0]
        x = self.patch_embed(x)
        x = x.flatten(2).transpose(1, 2)
        
        cls_tokens = self.cls_token.expand(B, -1, -1)
        x = torch.cat((cls_tokens, x), dim=1)
        x = x + self.pos_embed
        
        for block in self.blocks:
            x = block(x)
        
        x = self.norm(x)
        return x


CUSTOM_VIT_PATH = "/home/work/xueyunqi/11ar/comu_foren_dp/pretrained_weights/model_v11_ViT_224_base_ckpt.pt"

# æ–°å¢: UnivFD å’Œ DRCT æ¨¡å‹è·¯å¾„
UNIVFD_CHECKPOINT = "/home/work/xueyunqi/UniversalFakeDetect/pretrained_weights/fc_weights.pth"
DRCT_CONVNEXT_CHECKPOINT = "/home/work/xueyunqi/pretrained/DRCT-2M/sdv2/convnext_base_in22k_224_drct_amp_crop/16_acc0.9993.pth"

# è®¾å¤‡é…ç½®
DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {DEVICE}")

# ============================================================================
# Model Configuration
# ============================================================================
BASE_MODEL_PATH = "/home/work/xueyunqi/dinov3-vit7b16-pretrain-lvd1689m"
STUDENT_CHECKPOINT = "/home/work/xueyunqi/11ar/checkpoints/student/0917_dinov3_teacher_student/448_best_student_model.pth"
TEACHER_CHECKPOINT = "/home/work/xueyunqi/11ar/checkpoints/teacher/0917_dinov3_teacher_student/448_best_teacher_model.pth"

# æ–°å¢: AIDE æ¨¡å‹è·¯å¾„
AIDE_CHECKPOINT = "/home/work/xueyunqi/progan_train.pth"

DATASET_BASE_DIR = "/home/work/xueyunqi/11ar_datasets/test/progan"
REAL_ORIGINAL_DIR = os.path.join(DATASET_BASE_DIR, "sampled_images")
FAKE_ORIGINAL_DIR = os.path.join(DATASET_BASE_DIR, "1_fake")

OUTPUT_BASE_DIR = "attention_vision" # è¾“å‡ºæ–‡ä»¶å¤¹



# é…ç½®matplotlibä¸ºçº¿ç¨‹å®‰å…¨æ¨¡å¼
plt.switch_backend('Agg')
matplotlib_lock = Lock()

# Set matplotlib parameters for high quality
plt.rcParams.update({
    'font.family': 'serif',
    'font.serif': ['DejaVu Serif', 'Liberation Serif', 'Bitstream Vera Serif'],
    'font.size': 12,
    'axes.labelsize': 13, # ä¿®æ”¹
    'axes.titlesize': 16, # ä¿®æ”¹
    'xtick.labelsize': 11, # ä¿®æ”¹
    'ytick.labelsize': 11, # ä¿®æ”¹
    'legend.fontsize': 11,
    'figure.titlesize': 16,
    'text.usetex': False,
    'figure.dpi': 300,
    'savefig.dpi': 300,
    'savefig.bbox': 'tight',
    'savefig.pad_inches': 0.1
})

COL_DINO = "#E69F00"  # orange
COL_CLIP = "#56B4E9"  # blue
COL_CUSTOM_VIT = "#009E73"  # green (Community Forensics)
COL_UNIVFD = "#F0E442"  # yellow (UnivFD)
COL_DRCT_CONVB = "#CC79A7"  # pink/purple (DRCT ConvNeXt)
COL_AIDE = "#D55E00"  # red-orange (AIDE)


LINEWIDTH   = 2.2
MARKERSIZE  = 7
GRID_COLOR  = "#b0b0b0"   
GRID_ALPHA  = 0.35
GRID_LS     = "--"       
GRID_LW     = 0.8


def load_univfd_model():
    """
    åŠ è½½ UnivFD æ¨¡å‹ (åŸºäº CLIP ViT-L/14)
    """
    print(f"  Loading UnivFD model from: {UNIVFD_CHECKPOINT}")
    
    if not os.path.exists(UNIVFD_CHECKPOINT):
        print(f"  Error: Model file not found at {UNIVFD_CHECKPOINT}")
        return None, None
    
    try:
        # ä½¿ç”¨ open_clip åˆ›å»º CLIP ViT-L/14 æ¶æ„
        print(f"  Creating CLIP ViT-L/14 architecture for UnivFD...")
        model, _, preprocess = open_clip.create_model_and_transforms(
            "ViT-L-14", 
            pretrained="openai"  # ä½¿ç”¨ OpenAI çš„é¢„è®­ç»ƒæƒé‡ä½œä¸ºåŸºç¡€
        )
        print(f"  âœ“ Base model architecture created")
        
        # åŠ è½½ fc å±‚æƒé‡
        print(f"  Loading UnivFD fc weights...")
        fc_state_dict = torch.load(UNIVFD_CHECKPOINT, map_location='cpu')
        
        # UnivFD åªè®­ç»ƒäº†æœ€åçš„ fc å±‚,éœ€è¦æ·»åŠ è¿™ä¸ªåˆ†ç±»å¤´
        # æ ¹æ® validate.py çš„ä»£ç ,fc å±‚æ˜¯ä¸€ä¸ªç®€å•çš„çº¿æ€§å±‚
        import torch.nn as nn
        
        # è·å– CLIP çš„è¾“å‡ºç»´åº¦ (ViT-L/14 æ˜¯ 768)
        clip_output_dim = model.visual.output_dim
        
        # åˆ›å»º fc å±‚ (äºŒåˆ†ç±»)
        model.fc = nn.Linear(clip_output_dim, 1)
        
        # åŠ è½½ fc æƒé‡
        model.fc.load_state_dict(fc_state_dict)
        print(f"  âœ“ FC layer weights loaded")
        
        model = model.to(DEVICE)
        model.eval()
        print(f"  âœ“ UnivFD model loaded and moved to {DEVICE}")
        
        return model, preprocess
        
    except Exception as e:
        print(f"  Error loading UnivFD model: {e}")
        import traceback
        traceback.print_exc()
        return None, None


def load_drct_convnext_model():
    """
    åŠ è½½ DRCT ConvNeXt-Base æ¨¡å‹
    """
    print(f"  Loading DRCT ConvNeXt model from: {DRCT_CONVNEXT_CHECKPOINT}")
    
    if not os.path.exists(DRCT_CONVNEXT_CHECKPOINT):
        print(f"  Error: Model file not found at {DRCT_CONVNEXT_CHECKPOINT}")
        return None, None
    
    try:
        print(f"  Creating ConvNeXt-Base architecture...")
        
        base_model = timm.create_model(
            'convnext_base.fb_in22k',
            pretrained=False,
            num_classes=0
        )
        
        print(f"  âœ“ ConvNeXt-Base architecture created")
        
        # ğŸ”¥ æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°æ¨¡å‹ç»“æ„
        print(f"  Debug: ConvNeXt base model structure:")
        for name, module in base_model.named_children():
            print(f"    - {name}: {type(module)}")
        
        embedding_size = 1024
        num_classes = 2
        feature_dim = base_model.num_features
        print(f"  ConvNeXt feature dimension: {feature_dim}")
        
        class DRCTConvNeXt(nn.Module):
            def __init__(self, base_model, feature_dim, embedding_size, num_classes):
                super(DRCTConvNeXt, self).__init__()
                self.base_model = base_model
                
                if embedding_size is not None and embedding_size != feature_dim:
                    self.embedding = nn.Linear(feature_dim, embedding_size)
                    self.fc = nn.Linear(embedding_size, num_classes)
                else:
                    self.embedding = None
                    self.fc = nn.Linear(feature_dim, num_classes)
            
            def forward(self, x):
                features = self.base_model(x)
                if self.embedding is not None:
                    features = self.embedding(features)
                return self.fc(features)
            
            def forward_features(self, x):
                """æå–ç‰¹å¾ï¼ˆä¸ç»è¿‡åˆ†ç±»å¤´ï¼‰"""
                features = self.base_model(x)
                if self.embedding is not None:
                    features = self.embedding(features)
                return features
        
        model = DRCTConvNeXt(base_model, feature_dim, embedding_size, num_classes)
        print(f"  âœ“ DRCT ConvNeXt model structure created")
        
        # ğŸ”¥ æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°å®Œæ•´æ¨¡å‹ç»“æ„
        print(f"  Debug: Full model structure:")
        for name, module in model.named_children():
            print(f"    - {name}: {type(module)}")
            if name == 'base_model':
                for sub_name, sub_module in module.named_children():
                    print(f"      - {name}.{sub_name}: {type(sub_module)}")
        
        checkpoint = torch.load(DRCT_CONVNEXT_CHECKPOINT, map_location='cpu')
        print(f"  âœ“ Checkpoint loaded from disk")
        
        if isinstance(checkpoint, dict):
            if 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
            elif 'model' in checkpoint:
                state_dict = checkpoint['model']
            else:
                state_dict = checkpoint
        else:
            state_dict = checkpoint
        
        new_state_dict = {}
        for key, value in state_dict.items():
            if key.startswith('module.'):
                new_key = key[7:]
            else:
                new_key = key
            new_state_dict[new_key] = value
        
        missing_keys, unexpected_keys = model.load_state_dict(new_state_dict, strict=False)
        print(f"  âœ“ Model weights loaded")
        print(f"    Missing keys: {len(missing_keys)}")
        print(f"    Unexpected keys: {len(unexpected_keys)}")
        
        if len(missing_keys) > 0:
            print(f"    First 5 missing keys: {missing_keys[:5]}")
        if len(unexpected_keys) > 0:
            print(f"    First 5 unexpected keys: {unexpected_keys[:5]}")
        
        model = model.to(DEVICE)
        model.eval()
        print(f"  âœ“ DRCT ConvNeXt model loaded and moved to {DEVICE}")
        
        return model, None
        
    except Exception as e:
        print(f"  Error loading DRCT ConvNeXt model: {e}")
        import traceback
        traceback.print_exc()
        return None, None


# ğŸ”¥ ä¿®æ”¹ ConvNeXt ç‰¹å¾æå–å™¨ï¼Œæ·»åŠ æ›´å¤šè°ƒè¯•ä¿¡æ¯
class ConvNeXtFeatureExtractor:
    def __init__(self, model):
        self.model = model
        self.features = None
        self.hook = None
    
    def hook_fn(self, module, input, output):
        print(f"    Debug: Hook triggered, output shape: {output.shape}")
        self.features = output
    
    def register_hook(self, layer_name='base_model.stages.3'):
        """æ³¨å†Œhookåˆ°æŒ‡å®šå±‚"""
        try:
            target_layer = self.model
            for name in layer_name.split('.'):
                target_layer = getattr(target_layer, name)
                print(f"    Debug: Accessing layer {name}, type: {type(target_layer)}")
            
            self.hook = target_layer.register_forward_hook(self.hook_fn)
            print(f"    Debug: Hook registered to {layer_name}")
        except AttributeError as e:
            print(f"    Error: Cannot find layer {layer_name}: {e}")
            # å°è¯•å…¶ä»–å¯èƒ½çš„å±‚åç§°
            alternative_names = [
                'base_model.stages.3',
                'base_model.stages.-1',
                'base_model.norm',
                'base_model.head'
            ]
            for alt_name in alternative_names:
                try:
                    target_layer = self.model
                    for name in alt_name.split('.'):
                        target_layer = getattr(target_layer, name)
                    self.hook = target_layer.register_forward_hook(self.hook_fn)
                    print(f"    Debug: Successfully registered hook to alternative layer: {alt_name}")
                    break
                except:
                    continue
    
    def remove_hook(self):
        if self.hook is not None:
            self.hook.remove()
    
    def extract_features(self, x):
        print(f"    Debug: Input shape to ConvNeXt: {x.shape}")
        _ = self.model.forward_features(x)
        if self.features is not None:
            print(f"    Debug: Extracted features shape: {self.features.shape}")
        else:
            print(f"    Warning: No features extracted, hook may not have been triggered")
        return self.features



def load_aide_model():
    """åŠ è½½AIDEæ¨¡å‹"""
    print(f"  Loading AIDE model from: {AIDE_CHECKPOINT}")
    
    if not os.path.exists(AIDE_CHECKPOINT):
        print(f"  Error: Model file not found at {AIDE_CHECKPOINT}")
        return None, None
    
    try:
        checkpoint = torch.load(AIDE_CHECKPOINT, map_location='cpu')
        print(f"  âœ“ Checkpoint loaded from disk")
        
        # AIDEæ¨¡å‹æ¶æ„ (å‡è®¾æ˜¯ç±»ä¼¼Custom ViTçš„æ¶æ„ï¼Œæ‚¨å¯èƒ½éœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´)
        # å¦‚æœAIDEæœ‰ç‰¹å®šçš„æ¶æ„å®šä¹‰ï¼Œè¯·æ›¿æ¢è¿™éƒ¨åˆ†
        model = VisionTransformer(
            img_size=224, 
            patch_size=16, 
            embed_dim=768, 
            depth=12, 
            num_heads=12
        )
        
        # å¤„ç†checkpointæ ¼å¼
        if isinstance(checkpoint, dict):
            if 'model_state_dict' in checkpoint:
                state_dict = checkpoint['model_state_dict']
            elif 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
            elif 'model' in checkpoint:
                state_dict = checkpoint['model']
            else:
                state_dict = checkpoint
        else:
            state_dict = checkpoint
        
        # ç§»é™¤å¯èƒ½çš„ 'module.' å‰ç¼€
        new_state_dict = {}
        for key, value in state_dict.items():
            if key.startswith('module.'):
                new_key = key[7:]
            else:
                new_key = key
            new_state_dict[new_key] = value
        
        model.load_state_dict(new_state_dict, strict=False)
        
        model = model.to(DEVICE)
        model.eval()
        print(f"  âœ“ AIDE model loaded and moved to {DEVICE}")
        
        # AIDEä½¿ç”¨ä¸Custom ViTç›¸åŒçš„é¢„å¤„ç†
        return model, "custom"
        
    except Exception as e:
        print(f"  Error loading AIDE model: {e}")
        import traceback
        traceback.print_exc()
        return None, None



# ============================================================================
# Model Loading Functions - æ¥è‡ªä»£ç ä¸€
# ============================================================================
def load_model_from_checkpoint(checkpoint_path, base_model_path):
    """
    Load model from checkpoint file
    """
    print(f"  Loading model from checkpoint: {checkpoint_path}")
    
    if not os.path.exists(checkpoint_path):
        print(f"  Error: Checkpoint file not found at {checkpoint_path}")
        return None
    
    try:
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        print(f"  âœ“ Checkpoint loaded from disk")
    except Exception as e:
        print(f"  Error loading checkpoint: {e}")
        return None
    
    if 'model_state_dict' in checkpoint:
        state_dict = checkpoint['model_state_dict']
        print(f"  Found model_state_dict in checkpoint")
    else:
        print(f"  Error: No model_state_dict found in checkpoint")
        return None
    
    def remove_module_prefix(state_dict):
        new_state_dict = {}
        for key, value in state_dict.items():
            if key.startswith('module.'):
                new_key = key[7:]
                new_state_dict[new_key] = value
            else:
                new_state_dict[key] = value
        return new_state_dict
    
    state_dict = remove_module_prefix(state_dict)
    
    is_teacher_student = any(key.startswith(('teacher.', 'student_')) for key in state_dict.keys())
    
    if is_teacher_student:
        print(f"  Detected teacher-student model checkpoint")
        
        backbone_weights = {}
        prefix = 'teacher.backbone.'
        prefix_len = len(prefix)
        
        for key, value in state_dict.items():
            if key.startswith(prefix):
                backbone_key = key[prefix_len:]
                if len(backbone_weights) < 3:
                    print(f"    Debug: '{key}' -> '{backbone_key}'")
                backbone_weights[backbone_key] = value
        
        if not backbone_weights:
            print(f"  Warning: No backbone weights found in checkpoint")
            model = AutoModel.from_pretrained(base_model_path)
            model.eval()
            model = model.to(DEVICE)
            print(f"  âœ“ Model moved to {DEVICE}")
            return model
        
        print(f"  Found {len(backbone_weights)} backbone parameters")
        
        print(f"  Loading base model...")
        model = AutoModel.from_pretrained(base_model_path)
        print(f"  âœ“ Base model loaded")
        
        model_keys = list(model.state_dict().keys())[:3]
        print(f"    Model expects keys like: {model_keys}")
        
        print(f"  Loading backbone weights...")
        try:
            missing_keys, unexpected_keys = model.load_state_dict(backbone_weights, strict=False)
            print(f"  âœ“ Backbone weights loaded")
            print(f"    Missing keys: {len(missing_keys)}")
            print(f"    Unexpected keys: {len(unexpected_keys)}")
            
            if len(missing_keys) > 0:
                print(f"    First 3 missing keys: {missing_keys[:3]}")
            if len(unexpected_keys) > 0:
                print(f"    First 3 unexpected keys: {unexpected_keys[:3]}")
                
        except Exception as e:
            print(f"  Error loading backbone weights: {e}")
            import traceback
            traceback.print_exc()
            print(f"  Using base pretrained model instead...")
        
        del checkpoint
        del state_dict
        del backbone_weights
        torch.cuda.empty_cache()
        
        print(f"  Moving model to {DEVICE}...")
        model = model.to(DEVICE)
        print(f"  âœ“ Model moved to {DEVICE}")
    
    else:
        print(f"  Standard model checkpoint")
        model = AutoModel.from_pretrained(base_model_path)
        
        try:
            missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
            print(f"  âœ“ Model weights loaded")
        except Exception as e:
            print(f"  Warning: Error loading weights: {e}")
        
        model = model.to(DEVICE)
        print(f"  âœ“ Model moved to {DEVICE}")
    
    model.eval()
    print(f"  âœ“ Model set to eval mode")
    
    return model


def load_custom_vit_model():
    """åŠ è½½Custom ViTæ¨¡å‹"""
    print(f"  Loading Custom ViT from: {CUSTOM_VIT_PATH}")
    
    if not os.path.exists(CUSTOM_VIT_PATH):
        print(f"  Error: Model file not found at {CUSTOM_VIT_PATH}")
        return None, None
    
    try:
        checkpoint = torch.load(CUSTOM_VIT_PATH, map_location='cpu')
        print(f"  âœ“ Checkpoint loaded from disk")
        
        model = VisionTransformer(
            img_size=224, 
            patch_size=16, 
            embed_dim=768, 
            depth=12, 
            num_heads=12
        )
        
        if isinstance(checkpoint, dict):
            if 'model' in checkpoint:
                model.load_state_dict(checkpoint['model'], strict=False)
            elif 'state_dict' in checkpoint:
                model.load_state_dict(checkpoint['state_dict'], strict=False)
            else:
                model.load_state_dict(checkpoint, strict=False)
        else:
            model.load_state_dict(checkpoint, strict=False)
        
        model = model.to(DEVICE)
        model.eval()
        print(f"  âœ“ Custom ViT model loaded and moved to {DEVICE}")
        
        # Custom ViTä½¿ç”¨è‡ªå·±çš„é¢„å¤„ç†
        return model, "custom"
        
    except Exception as e:
        print(f"  Error loading Custom ViT model: {e}")
        import traceback
        traceback.print_exc()
        return None, None


def load_clip_model():
    """åŠ è½½CLIPæ¨¡å‹"""
    print(f"  Loading CLIP ViT-H/14 (LAION-2B)...")
    
    try:
        model, _, preprocess = open_clip.create_model_and_transforms(
            "ViT-H-14", pretrained="laion2b_s32b_b79k"
        )
        model = model.to(DEVICE)
        model.eval()
        print(f"  âœ“ CLIP model loaded and moved to {DEVICE}")
        
        return model, preprocess
        
    except Exception as e:
        print(f"  Error loading CLIP model: {e}")
        import traceback
        traceback.print_exc()
        return None, None

# ============================================================================
# Image Collection Functions - æ¥è‡ªä»£ç ä¸€
# ============================================================================
# ============================================================================
# Image Collection Functions - ä¿®æ”¹ä¸ºè¯»å–æ‰€æœ‰å›¾ç‰‡
# ============================================================================
def get_image_files_from_directory(folder_path, num_images=None):
    """
    è·å–æŒ‡å®šæ–‡ä»¶å¤¹ä¸­çš„å›¾ç‰‡æ–‡ä»¶
    
    Args:
        folder_path: æ–‡ä»¶å¤¹è·¯å¾„
        num_images: è¦è¯»å–çš„å›¾ç‰‡æ•°é‡ï¼Œå¦‚æœä¸ºNoneåˆ™è¯»å–æ‰€æœ‰å›¾ç‰‡
    
    Returns:
        list: å›¾ç‰‡æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    image_files = []
    folder = Path(folder_path)
    
    if not folder.exists():
        print(f"Error: Folder not found at {folder_path}")
        return []
    
    print(f"Collecting images from {folder_path}...")
    
    # æ”¯æŒçš„å›¾ç‰‡æ‰©å±•å
    valid_extensions = ['.jpg', '.png', '.jpeg', '.JPG', '.PNG', '.JPEG', '.bmp', '.BMP']
    
    # æ”¶é›†æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
    all_images = []
    for ext in valid_extensions:
        all_images.extend(list(folder.glob(f'*{ext}')))
    
    # æŒ‰æ–‡ä»¶åæ’åº
    all_images = sorted(all_images, key=lambda x: x.name)
    
    # å¦‚æœæŒ‡å®šäº†æ•°é‡é™åˆ¶
    if num_images is not None:
        all_images = all_images[:num_images]
        print(f"  Limiting to first {num_images} images")
    
    for img_path in all_images:
        image_files.append(img_path)
        if len(image_files) <= 5 or len(image_files) == len(all_images):  # åªæ‰“å°å‰5å¼ å’Œæœ€åä¸€å¼ 
            print(f"  âœ“ Found image: {img_path.name}")
        elif len(image_files) == 6:
            print(f"  ... (showing first 5 and last)")
    
    print(f"âœ“ Found {len(image_files)} images in {folder_path}")
    return image_files


# ============================================================================
# Motion Blur Functions - æ¥è‡ªä»£ç ä¸€
# ============================================================================
def motion_blur(img, kernel_size=15, angle=0):
    """
    å¯¹è¾“å…¥å›¾ç‰‡æ·»åŠ è¿åŠ¨æ¨¡ç³Šæ•ˆæœ
    
    Args:
        img (np.ndarray): è¾“å…¥çš„å›¾ç‰‡æ•°ç»„ï¼ˆBGRæ ¼å¼ï¼‰
        kernel_size (int): æ¨¡ç³Šæ ¸çš„å¤§å°ï¼Œå†³å®šæ¨¡ç³Šå¼ºåº¦
        angle (float): æ¨¡ç³Šæ–¹å‘çš„è§’åº¦ï¼ˆåº¦ï¼‰ï¼Œ0ä¸ºæ°´å¹³
    
    Returns:
        np.ndarray: æ·»åŠ è¿åŠ¨æ¨¡ç³Šåçš„å›¾ç‰‡æ•°ç»„
    """
    kernel = np.zeros((kernel_size, kernel_size))
    kernel[int((kernel_size-1)/2), :] = np.ones(kernel_size)
    M = cv2.getRotationMatrix2D((kernel_size/2-0.5, kernel_size/2-0.5), angle, 1)
    kernel = cv2.warpAffine(kernel, M, (kernel_size, kernel_size))
    kernel = kernel / np.sum(kernel)
    blurred = cv2.filter2D(img, -1, kernel)
    return blurred


def strength_to_kernel_size(strength, base_size=15):
    """å°†æ¨¡ç³Šå¼ºåº¦è½¬æ¢ä¸ºkernel_size"""
    # strengthèŒƒå›´0.01-0.10ï¼Œæ˜ å°„åˆ°kernel_size
    min_kernel = 3
    max_kernel = 15
    kernel_size = int(min_kernel + (strength - 0.01) * (max_kernel - min_kernel) / 0.09)
    return kernel_size if kernel_size % 2 == 1 else kernel_size + 1


def apply_motion_blur_to_pil_image(image_pil, strength, angle=0):
    """å¯¹PILå›¾åƒåº”ç”¨è¿åŠ¨æ¨¡ç³Š"""
    image_cv = cv2.cvtColor(np.array(image_pil), cv2.COLOR_RGB2BGR)
    kernel_size = strength_to_kernel_size(strength)
    blurred_cv = motion_blur(image_cv, kernel_size=kernel_size, angle=angle)
    blurred_pil = Image.fromarray(cv2.cvtColor(blurred_cv, cv2.COLOR_BGR2RGB))
    return blurred_pil

# ============================================================================
# æ–°å¢: å­é›†åˆ†æè¾…åŠ©å‡½æ•°
# ============================================================================
def analyze_subset(model, processor, image_files, model_name, model_type, 
                   clip_preprocess, blur_strengths, subset_name="subset"):
    """
    å¯¹å›¾ç‰‡å­é›†è¿›è¡Œè¿åŠ¨æ¨¡ç³Šåˆ†æ
    """
    print(f"{'='*80}")
    print(f"Analyzing {subset_name} subset - {len(image_files)} images")
    print('='*80)
    
    # å­˜å‚¨æ‰€æœ‰å›¾ç‰‡çš„ç›¸ä¼¼åº¦æ•°æ®
    all_image_similarities = {}
    
    for img_idx, image_path in enumerate(image_files):
        image_name = image_path.stem
        
        # æ¯10å¼ å›¾ç‰‡æ‰“å°ä¸€æ¬¡è¿›åº¦
        if (img_idx + 1) % 10 == 0 or img_idx == 0 or img_idx == len(image_files) - 1:
            print(f"[{img_idx+1}/{len(image_files)}] Processing: {image_name}")
        
        try:
            original_image = Image.open(image_path).convert('RGB')
            original_size = original_image.size
            
            # ğŸ”¥ æ·»åŠ è¯¦ç»†çš„é”™è¯¯è¿½è¸ª
            try:
                # è®¡ç®—åŸå§‹å›¾ç‰‡çš„æ³¨æ„åŠ›å›¾
                original_image_tensor, _, _ = preprocess_image_for_blur_analysis(
                    original_image, model_type, processor, clip_preprocess
                )
                
                if original_image_tensor is None:
                    print(f"    âš  Warning: Failed to preprocess {image_name}, skipping.")
                    continue
                
                print(f"    Debug: Preprocessed tensor shape: {original_image_tensor.shape}")
                
            except Exception as e:
                print(f"    âœ— Error in preprocessing {image_name}: {e}")
                import traceback
                traceback.print_exc()
                continue
            
            try:
                original_attention_map = compute_multi_scale_attention(
                    model, original_image_tensor, model_type
                )
                
                if original_attention_map is None:
                    print(f"    âš  Warning: Failed to compute attention for {image_name}, skipping.")
                    continue
                
                print(f"    Debug: Attention map shape: {original_attention_map.shape}, max: {original_attention_map.max()}, min: {original_attention_map.min()}")
                
            except Exception as e:
                print(f"    âœ— Error in computing attention for {image_name}: {e}")
                import traceback
                traceback.print_exc()
                continue
            
            if original_attention_map.max() == 0:
                print(f"    âš  Warning: Original attention map for {image_name} is all zeros! Skipping.")
                continue

            image_similarities = {}
            
            for strength in blur_strengths:
                if strength == 0.0:
                    test_image = original_image
                    attention_map = original_attention_map
                else:
                    test_image = apply_motion_blur_to_pil_image(original_image, strength, BLUR_ANGLE)
                    image_tensor, _, _ = preprocess_image_for_blur_analysis(
                        test_image, model_type, processor, clip_preprocess
                    )
                    
                    if image_tensor is None:
                        print(f"    âš  Warning: Failed to preprocess blurred image at strength {strength}")
                        continue
                    
                    attention_map = compute_multi_scale_attention(model, image_tensor, model_type)
                    
                    if attention_map is None:
                        print(f"    âš  Warning: Failed to compute attention for blurred image at strength {strength}")
                        continue
                
                # è®¡ç®—ç›¸ä¼¼åº¦
                if attention_map.max() == 0:
                    similarity = 0.0
                else:
                    correlation = np.corrcoef(
                        original_attention_map.flatten(), 
                        attention_map.flatten()
                    )[0, 1]
                    similarity = correlation if not np.isnan(correlation) else 0.0
                
                image_similarities[f'{strength:.3f}'] = float(similarity)
            
            all_image_similarities[image_name] = image_similarities
            
        except Exception as e:
            print(f"  âœ— Error analyzing {image_name}: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # è®¡ç®—å¹³å‡ç›¸ä¼¼åº¦
    subset_avg_similarities = {}
    for strength in blur_strengths:
        strength_str = f'{strength:.3f}'
        similarities_at_strength = []
        for img_name, img_data in all_image_similarities.items():
            if strength_str in img_data:
                similarities_at_strength.append(img_data[strength_str])
        
        if similarities_at_strength:
            avg_similarity = np.mean(similarities_at_strength)
            subset_avg_similarities[strength_str] = float(avg_similarity)
        else:
            subset_avg_similarities[strength_str] = 0.0
    
    print(f"âœ“ Completed analysis for {subset_name} subset")
    print(f"  Processed {len(all_image_similarities)}/{len(image_files)} images successfully")
    
    return subset_avg_similarities, all_image_similarities

def preprocess_drct_convnext(image):
    """DRCT ConvNeXté¢„å¤„ç† - ä½¿ç”¨ImageNetæ ‡å‡†"""
    from torchvision import transforms
    
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    img_tensor = transform(image).unsqueeze(0)
    return img_tensor
# ============================================================================
# Image Preprocessing Functions
# ============================================================================
def make_transform_for_dinov3(processor):
    """åˆ›å»ºä½¿ç”¨AutoImageProcessorçš„transform"""
    def transform(image):
        inputs = processor(images=image, return_tensors="pt")
        return inputs['pixel_values'].squeeze(0)
    return transform

def preprocess_images_batch(image_paths, model_type, processor=None, clip_preprocess=None):
    """æ‰¹é‡é¢„å¤„ç†å›¾åƒ - æ”¯æŒå¤šç§æ¨¡å‹ç±»å‹"""
    print(f"Preprocessing {len(image_paths)} images for model type: {model_type}...")
    
    batch_tensors = []
    original_images = []
    original_sizes = []
    image_names = []
    
    for image_path in image_paths:
        try:
            image = Image.open(image_path).convert('RGB')
            original_size = image.size
            
            # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©é¢„å¤„ç†æ–¹æ³•
            if model_type == "dinov3":
                inputs = processor(images=image, return_tensors="pt")
                processed_image = inputs['pixel_values'].squeeze(0)
            elif model_type == "custom":
                processed_image = preprocess_custom_vit(image).squeeze(0)
            elif model_type == "clip":
                processed_image = clip_preprocess(image)
            else:
                raise ValueError(f"Unknown model type: {model_type}")
            
            batch_tensors.append(processed_image)
            original_images.append(image)
            original_sizes.append(original_size)
            image_names.append(image_path.stem)
            
        except Exception as e:
            print(f"Error processing {image_path}: {e}")
            continue
    
    if batch_tensors:
        batch_tensor = torch.stack(batch_tensors)
        print(f"Created batch tensor: {batch_tensor.shape}")
        return batch_tensor, original_images, original_sizes, image_names
    else:
        return None, [], [], []


def preprocess_custom_vit(image):
    """Custom ViTé¢„å¤„ç†"""
    img = image.resize((224, 224))
    img_array = np.array(img).astype(np.float32) / 255.0
    mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)
    std = np.array([0.229, 0.224, 0.225], dtype=np.float32)
    img_array = (img_array - mean) / std
    img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0).float()
    return img_tensor

def preprocess_image_for_blur_analysis(image_pil, model_type, processor=None, clip_preprocess=None):
    """ä¸ºè¿åŠ¨æ¨¡ç³Šåˆ†æé¢„å¤„ç†å›¾åƒ - æ”¯æŒå¤šç§æ¨¡å‹"""
    try:
        if model_type == "dinov3":
            inputs = processor(images=image_pil, return_tensors="pt")
            processed_image = inputs['pixel_values']
            
        elif model_type == "custom" or model_type == "aide":
            processed_image = preprocess_custom_vit(image_pil)
            
        elif model_type == "clip" or model_type == "univfd":  # ä¿®æ”¹è¿™é‡Œ
            if clip_preprocess is None:
                print(f"    âš  Warning: clip_preprocess is None for {model_type}")
                return None, image_pil, image_pil.size
            processed_image = clip_preprocess(image_pil).unsqueeze(0)

        elif model_type == "drct_convnext":  # æ–°å¢
            processed_image = preprocess_drct_convnext(image_pil)

        else:
            raise ValueError(f"Unknown model type: {model_type}")
        
        # æ£€æŸ¥å¤„ç†åçš„å›¾åƒ
        if processed_image is None or processed_image.numel() == 0:
            print(f"    âš  Warning: Preprocessing resulted in empty tensor for {model_type}")
            return None, image_pil, image_pil.size
        
        return processed_image, image_pil, image_pil.size
        
    except Exception as e:
        print(f"    âœ— Error in preprocessing for {model_type}: {e}")
        import traceback
        traceback.print_exc()
        return None, image_pil, image_pil.size




# ============================================================================
# Feature Extraction Functions - ä¿®æ”¹ä¸ºä½¿ç”¨transformersæ¨¡å‹
# ============================================================================
def extract_dense_features(model, image_tensor, model_type, processor=None):
    """æå–dense features - æ”¯æŒDINOv3ã€Custom ViTã€CLIPã€UnivFDã€DRCTå’ŒAIDE"""
    print("Extracting dense features...")
    
    device = next(model.parameters()).device
    image_tensor = image_tensor.to(device)
    
    with torch.no_grad():
        if model_type == "dinov3":
            # DINOv3æ¨¡å‹
            outputs = model(pixel_values=image_tensor)
            num_register = model.config.num_register_tokens
            patch_features = outputs.last_hidden_state[:, 1 + num_register:, :]
            patch_features = F.normalize(patch_features, dim=-1)
            
            batch_size, num_patches, feature_dim = patch_features.shape
            patch_size = int(np.sqrt(num_patches))
            
        elif model_type == "custom" or model_type == "aide":
            # Custom ViTæ¨¡å‹ å’Œ AIDE
            outputs = model(image_tensor)
            patch_features = outputs[:, 1:, :]  # å»æ‰CLS token
            patch_features = F.normalize(patch_features, dim=-1)
            
            batch_size, num_patches, feature_dim = patch_features.shape
            patch_size = int(np.sqrt(num_patches))
            
        elif model_type == "clip" or model_type == "univfd" or model_type == "drct":  # ä¿®æ”¹è¿™é‡Œ
            # CLIPæ¨¡å‹ã€UnivFD å’Œ DRCT
            try:
                x = model.visual.conv1(image_tensor)
                x = x.reshape(x.shape[0], x.shape[1], -1)
                x = x.permute(0, 2, 1)
                
                cls_token = model.visual.class_embedding.unsqueeze(0).expand(x.shape[0], -1, -1)
                x = torch.cat([cls_token, x], dim=1)
                x = x + model.visual.positional_embedding
                x = model.visual.ln_pre(x)
                
                x = x.permute(1, 0, 2)
                x = model.visual.transformer(x)
                x = x.permute(1, 0, 2)
                
                patch_features = x[:, 1:, :]  # å»æ‰CLS token
                
                # æ·»åŠ ç©ºå€¼æ£€æŸ¥
                if patch_features.numel() == 0:
                    print(f"  âš  Warning: Empty patch features from {model_type} model!")
                    return None, 0
                
                patch_features = F.normalize(patch_features, dim=-1)
                
                batch_size, num_patches, feature_dim = patch_features.shape
                patch_size = int(np.sqrt(num_patches))
                
            except Exception as e:
                print(f"  âœ— Error extracting features from {model_type}: {e}")
                return None, 0
        
        else:
            raise ValueError(f"Unknown model type: {model_type}")
        
        print(f"Feature shape: {patch_features.shape}")
        print(f"Patch grid size: {patch_size}x{patch_size}")
        print(f"Feature dimension: {feature_dim}")
        
        # é‡å¡‘ä¸º2D grid
        dense_features = patch_features.reshape(batch_size, patch_size, patch_size, feature_dim)
        dense_features = dense_features.permute(0, 3, 1, 2)
        
    return dense_features, patch_size

# ============================================================================
# Feature Extraction Functions - æ”¯æŒå¤šç§æ¨¡å‹ç±»å‹ (ç»­)
# ============================================================================
def extract_dense_features_batch(model, batch_tensor, model_type, processor=None):
    """æ‰¹é‡æå–dense features - æ”¯æŒå¤šç§æ¨¡å‹"""
    print(f"Extracting dense features for batch of {batch_tensor.shape[0]} images...")
    
    device = next(model.parameters()).device
    batch_tensor = batch_tensor.to(device)
    
    with torch.no_grad():
        if model_type == "dinov3":
            outputs = model(pixel_values=batch_tensor)
            num_register = model.config.num_register_tokens
            patch_features = outputs.last_hidden_state[:, 1 + num_register:, :]
            patch_features = F.normalize(patch_features, dim=-1)
            
        elif model_type == "custom":
            outputs = model(batch_tensor)
            patch_features = outputs[:, 1:, :]
            patch_features = F.normalize(patch_features, dim=-1)
            
        elif model_type == "clip":
            x = model.visual.conv1(batch_tensor)
            x = x.reshape(x.shape[0], x.shape[1], -1)
            x = x.permute(0, 2, 1)
            
            cls_token = model.visual.class_embedding.unsqueeze(0).expand(x.shape[0], -1, -1)
            x = torch.cat([cls_token, x], dim=1)
            x = x + model.visual.positional_embedding
            x = model.visual.ln_pre(x)
            
            x = x.permute(1, 0, 2)
            x = model.visual.transformer(x)
            x = x.permute(1, 0, 2)
            
            patch_features = x[:, 1:, :]
            patch_features = F.normalize(patch_features, dim=-1)
        
        else:
            raise ValueError(f"Unknown model type: {model_type}") # è¡¥å…¨è¿™ä¸€è¡Œ
        
        batch_size, num_patches, feature_dim = patch_features.shape
        patch_size = int(np.sqrt(num_patches))
        
        print(f"Batch feature shape: {patch_features.shape}")
        print(f"Patch grid size: {patch_size}x{patch_size}")
        print(f"Feature dimension: {feature_dim}")
        
        dense_features = patch_features.reshape(batch_size, patch_size, patch_size, feature_dim)
        dense_features = dense_features.permute(0, 3, 1, 2)
        
    return dense_features, patch_size


# ============================================================================
# Attention Map Functions - æ”¯æŒå¤šç§æ¨¡å‹ç±»å‹
# ============================================================================
def get_gradcam_attention_map(model, image_tensor, model_type, target_layer_idx=-1):
    """
    ä½¿ç”¨GradCAMæ–¹æ³•è®¡ç®—æ³¨æ„åŠ›å›¾ - é€‚é…transformersã€Custom ViTå’ŒCLIPæ¨¡å‹
    """
    model.eval()
    
    device = next(model.parameters()).device
    image_tensor = image_tensor.to(device)
    
    features = []
    gradients = []
    
    def forward_hook(module, input, output):
        features.append(output)
    
    def backward_hook(module, grad_input, grad_output):
        gradients.append(grad_output[0])
    
    # æ³¨å†Œhook
    try:
        if model_type == "dinov3":
            # ğŸ”¥ ä¿®æ”¹ï¼šDINOv3 çš„æ­£ç¡®å±‚çº§ç»“æ„
            # DINOv3 ä½¿ç”¨ model.encoder.layer æˆ– model.blocks
            if hasattr(model, 'encoder') and hasattr(model.encoder, 'layer'):
                target_layer = model.encoder.layer[target_layer_idx]
            elif hasattr(model, 'blocks'):
                target_layer = model.blocks[target_layer_idx]
            else:
                # å¦‚æœéƒ½æ²¡æœ‰ï¼Œå°è¯•æ‰¾åˆ°æ‰€æœ‰çš„ transformer blocks
                print(f"  Warning: Cannot find standard transformer layers in DINOv3, using feature_norm method")
                return compute_attention_map(model, image_tensor, model_type, method='feature_norm')
                
        elif model_type == "custom" or model_type == "aide":
            # Custom ViT: æ³¨å†Œåˆ°æœ€åä¸€ä¸ªtransformer block
            target_layer = model.blocks[target_layer_idx]
            
        elif model_type == "clip" or model_type == "effort":
            # CLIP: æ³¨å†Œåˆ°æœ€åä¸€ä¸ªresblock
            target_layer = model.visual.transformer.resblocks[target_layer_idx]
        else:
            raise ValueError(f"Unknown model type for GradCAM: {model_type}")
            
        forward_handle = target_layer.register_forward_hook(forward_hook)
        backward_handle = target_layer.register_full_backward_hook(backward_hook)
        
    except Exception as e:
        print(f"  Warning: Could not register hooks for {model_type} GradCAM ({e}), using feature_norm method")
        return compute_attention_map(model, image_tensor, model_type, method='feature_norm')
    
    model.zero_grad()
    
    # Forward pass
    if model_type == "dinov3":
        outputs = model(pixel_values=image_tensor)
        # DINOv3çš„CLS tokenåœ¨outputs.last_hidden_stateçš„ç´¢å¼•0
        cls_token = outputs.last_hidden_state[:, 0, :]
    elif model_type == "custom":
        outputs = model(image_tensor)
        cls_token = outputs[:, 0, :]
    elif model_type == "clip":
        x = model.visual.conv1(image_tensor)
        x = x.reshape(x.shape[0], x.shape[1], -1)
        x = x.permute(0, 2, 1)
        
        cls_token_embed = model.visual.class_embedding.unsqueeze(0).expand(x.shape[0], -1, -1)
        x = torch.cat([cls_token_embed, x], dim=1)
        x = x + model.visual.positional_embedding
        x = model.visual.ln_pre(x)
        
        x = x.permute(1, 0, 2)
        x = model.visual.transformer(x)
        x = x.permute(1, 0, 2)
        
        cls_token = x[:, 0, :] # CLIPçš„CLS token
    
    target = cls_token.sum()
    target.backward()
    
    forward_handle.remove()
    backward_handle.remove()
    
    if len(features) == 0 or len(gradients) == 0:
        print(f"Warning: No gradients captured for {model_type} GradCAM, using feature magnitude method")
        return compute_attention_map(model, image_tensor, model_type, method='feature_norm')
    
    feature = features[0]
    gradient = gradients[0]
    
    # å¤„ç†ç‰¹å¾å’Œæ¢¯åº¦ï¼Œå»é™¤CLS tokenå’Œå¯èƒ½çš„register tokens
    if model_type == "dinov3":
        num_register = model.config.num_register_tokens
        feature = feature[:, 1 + num_register:, :]
        gradient = gradient[:, 1 + num_register:, :]
    elif model_type == "custom":
        feature = feature[:, 1:, :] # Custom ViT
        gradient = gradient[:, 1:, :]
    elif model_type == "clip":
        # CLIPçš„transformerè¾“å‡ºå¯èƒ½æ˜¯[seq_len, batch, dim]
        if feature.dim() == 3 and feature.shape[0] == feature.shape[1]: # å‡è®¾æ˜¯[seq_len, batch, dim]
             feature = feature.permute(1, 0, 2)  # [batch, seq_len, dim]
             gradient = gradient.permute(1, 0, 2)
        
        feature = feature[:, 1:, :]  # CLIP
        gradient = gradient[:, 1:, :]
    
    weights = gradient.mean(dim=1, keepdim=True)
    cam = (weights * feature).sum(dim=2)
    cam = torch.relu(cam)
    cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)
    
    num_patches = cam.shape[1]
    patch_size = int(np.sqrt(num_patches))
    attention_map = cam.reshape(patch_size, patch_size)
    
    return attention_map.cpu().detach().numpy()

def compute_attention_map(model, image_tensor, model_type, method='feature_norm'):
    """
    è®¡ç®—æ³¨æ„åŠ›å›¾
    """
    device = next(model.parameters()).device
    image_tensor = image_tensor.to(device)
    
    with torch.no_grad():
        if model_type == "dinov3":
            # DINOv3 å¤„ç†é€»è¾‘
            outputs = model(pixel_values=image_tensor)
            num_register = model.config.num_register_tokens
            patch_features = outputs.last_hidden_state[:, 1 + num_register:, :]
            
        elif model_type == "custom" or model_type == "aide":
            # Custom ViT å’Œ AIDE å¤„ç†é€»è¾‘
            outputs = model(image_tensor)
            patch_features = outputs[:, 1:, :]
            
        elif model_type == "drct_convnext":
            # ğŸ”¥ DRCT ConvNeXt çš„ç‰¹æ®Šå¤„ç†
            try:
                extractor = ConvNeXtFeatureExtractor(model)
                extractor.register_hook(layer_name='base_model.stages.3')
                
                features = extractor.extract_features(image_tensor)  # [B, C, H, W]
                extractor.remove_hook()
                
                if features.dim() == 4:
                    B, C, H, W = features.shape
                    print(f"    Debug: ConvNeXt features shape: {features.shape}")
                    
                    if method == 'feature_norm':
                        # è®¡ç®—ç‰¹å¾èŒƒæ•°
                        feature_norm = torch.norm(features, dim=1, keepdim=False)  # [B, H, W]
                        attention_map = feature_norm.squeeze(0)  # [H, W]
                        
                        # å½’ä¸€åŒ–
                        att_min = attention_map.min()
                        att_max = attention_map.max()
                        
                        if att_max == att_min:
                            print(f"    âš  Warning: Uniform attention map for drct_convnext, using 0.5")
                            attention_map = torch.ones_like(attention_map) * 0.5
                        else:
                            attention_map = (attention_map - att_min) / (att_max - att_min + 1e-8)
                        
                        print(f"    Debug: ConvNeXt attention map shape: {attention_map.shape}, max: {attention_map.max():.4f}, min: {attention_map.min():.4f}")
                        return attention_map.cpu().numpy()
                    
                    else:
                        # å¯¹äºå…¶ä»–æ–¹æ³•ï¼Œå°†ConvNeXtç‰¹å¾è½¬æ¢ä¸ºpatchæ ¼å¼
                        patch_features = features.reshape(B, C, H*W).permute(0, 2, 1)  # [B, H*W, C]
                        print(f"    Debug: ConvNeXt patch_features shape: {patch_features.shape}")
                        
                else:
                    print(f"    âš  Unexpected ConvNeXt feature shape: {features.shape}")
                    return np.zeros((7, 7))
                    
            except Exception as e:
                print(f"    âœ— Error computing ConvNeXt attention: {e}")
                import traceback
                traceback.print_exc()
                return np.zeros((7, 7))
            
        elif model_type == "clip" or model_type == "univfd" or model_type == "drct":
            # CLIPã€UnivFD å’Œ DRCT å¤„ç†é€»è¾‘
            x = model.visual.conv1(image_tensor)
            x = x.reshape(x.shape[0], x.shape[1], -1)
            x = x.permute(0, 2, 1)
            
            cls_token = model.visual.class_embedding.unsqueeze(0).expand(x.shape[0], -1, -1)
            x = torch.cat([cls_token, x], dim=1)
            x = x + model.visual.positional_embedding
            x = model.visual.ln_pre(x)
            
            x = x.permute(1, 0, 2)
            x = model.visual.transformer(x)
            x = x.permute(1, 0, 2)
            
            patch_features = x[:, 1:, :]
            
        else:
            raise ValueError(f"Unknown model type: {model_type}")
        
        # ğŸ”¥ ç¡®ä¿ patch_features å­˜åœ¨åå†è¿›è¡Œåç»­å¤„ç†
        if 'patch_features' not in locals():
            print(f"    âš  Warning: patch_features not defined for {model_type}")
            patch_size = 16 if model_type in ["clip", "univfd", "drct"] else (7 if model_type == "drct_convnext" else 14)
            return np.zeros((patch_size, patch_size))
        
        if patch_features.numel() == 0:
            print(f"    âš  Warning: Empty patch features for {model_type}")
            patch_size = 16 if model_type in ["clip", "univfd", "drct"] else 14
            return np.zeros((patch_size, patch_size))
        
        # æ ¹æ®æ–¹æ³•è®¡ç®—æ³¨æ„åŠ›å›¾
        if method == 'feature_norm':
            # è®¡ç®—ç‰¹å¾èŒƒæ•°
            feature_norm = torch.norm(patch_features, dim=-1)  # [B, N]
            attention_weights = feature_norm.squeeze(0)  # [N]
            
        elif method == 'mean_pooling':
            # å¹³å‡æ± åŒ–
            attention_weights = torch.mean(patch_features, dim=-1).squeeze(0)  # [N]
            
        else:
            raise ValueError(f"Unknown method: {method}")
        
        # é‡å¡‘ä¸º2D attention map
        num_patches = attention_weights.shape[0]
        patch_size = int(np.sqrt(num_patches))
        attention_map = attention_weights.reshape(patch_size, patch_size)
        
        # å½’ä¸€åŒ–
        att_min = attention_map.min()
        att_max = attention_map.max()
        
        if att_max == att_min:
            print(f"    âš  Warning: Uniform attention map for {model_type}, using 0.5")
            attention_map = torch.ones_like(attention_map) * 0.5
        else:
            attention_map = (attention_map - att_min) / (att_max - att_min + 1e-8)
        
        print(f"    Debug: Final attention map shape: {attention_map.shape}, max: {attention_map.max():.4f}, min: {attention_map.min():.4f}")
        return attention_map.cpu().numpy()



def get_attention_map_alternative(model, image_tensor, model_type):
    """
    å¤‡ç”¨æ–¹æ³•ï¼šä½¿ç”¨æœ€åä¸€å±‚çš„æ³¨æ„åŠ›æƒé‡æˆ–CLS tokenä¸patchçš„ç›¸ä¼¼åº¦ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    """
    device = next(model.parameters()).device
    image_tensor = image_tensor.to(device)
    
    with torch.no_grad():
        if model_type == "dinov3":
            return compute_attention_map(model, image_tensor, model_type, method='feature_norm')
            
        elif model_type == "custom" or model_type == "aide":
            try:
                outputs = model(image_tensor)
                patch_features = outputs[:, 1:, :]
                
                if patch_features.numel() == 0:
                    print(f"    âš  Warning: Empty patch features in alternative method for {model_type}")
                    return np.zeros((14, 14))
                
                patch_features_norm = F.normalize(patch_features, dim=-1)
                similarity = torch.matmul(patch_features_norm, patch_features_norm.transpose(1, 2))
                attention_weights = similarity.mean(dim=1)
                
            except Exception as e:
                print(f"    âœ— Error in alternative method for {model_type}: {e}")
                return np.zeros((14, 14))
        
        # æ·»åŠ  drct_convnext çš„å¤„ç†
        elif model_type == "drct_convnext":
            try:
                # ä½¿ç”¨hookæå–ä¸­é—´ç‰¹å¾
                extractor = ConvNeXtFeatureExtractor(model)
                extractor.register_hook(layer_name='base_model.stages.3')
                
                features = extractor.extract_features(image_tensor)  # [B, C, H, W]
                extractor.remove_hook()
                
                if features.dim() == 4:
                    B, C, H, W = features.shape
                    # å°†ç©ºé—´ç‰¹å¾å±•å¹³ä¸ºpatches
                    patch_features = features.reshape(B, C, H*W).permute(0, 2, 1)  # [B, H*W, C]
                    
                    if patch_features.numel() == 0:
                        print(f"    âš  Warning: Empty patch features for drct_convnext")
                        return np.zeros((H, W))
                    
                    # è®¡ç®—patchä¹‹é—´çš„ç›¸ä¼¼åº¦
                    patch_features_norm = F.normalize(patch_features, dim=-1)
                    similarity = torch.matmul(patch_features_norm, patch_features_norm.transpose(1, 2))
                    attention_weights = similarity.mean(dim=1)  # [B, H*W]
                    
                    # é‡å¡‘ä¸º2D
                    attention_map = attention_weights.reshape(H, W)
                else:
                    print(f"    âš  Unexpected feature shape: {features.shape}")
                    return np.zeros((7, 7))
                    
            except Exception as e:
                print(f"    âœ— Error in alternative method for drct_convnext: {e}")
                import traceback
                traceback.print_exc()
                return np.zeros((7, 7))
            
        elif model_type == "clip" or model_type == "univfd" or model_type == "drct":
            # ä¿æŒåŸæœ‰çš„CLIP/UnivFD/DRCTå¤„ç†é€»è¾‘
            try:
                x = model.visual.conv1(image_tensor)
                x = x.reshape(x.shape[0], x.shape[1], -1)
                x = x.permute(0, 2, 1)
                
                cls_token = model.visual.class_embedding.unsqueeze(0).expand(x.shape[0], -1, -1)
                x = torch.cat([cls_token, x], dim=1)
                x = x + model.visual.positional_embedding
                x = model.visual.ln_pre(x)
                
                x = x.permute(1, 0, 2)
                x = model.visual.transformer(x)
                x = x.permute(1, 0, 2)
                
                if x.numel() == 0:
                    print(f"    âš  Warning: Empty transformer output for {model_type}")
                    return np.zeros((16, 16))
                
                cls_feature = x[:, 0:1, :]
                patch_features = x[:, 1:, :]
                
                if patch_features.numel() == 0:
                    print(f"    âš  Warning: Empty patch features for {model_type}")
                    return np.zeros((16, 16))
                
                cls_norm = F.normalize(cls_feature, dim=-1)
                patch_norm = F.normalize(patch_features, dim=-1)
                
                attention_weights = torch.matmul(cls_norm, patch_norm.transpose(1, 2)).squeeze(1)
                
            except Exception as e:
                print(f"    âœ— Error in alternative method for {model_type}: {e}")
                import traceback
                traceback.print_exc()
                return np.zeros((16, 16))
        
        else:
            raise ValueError(f"Unknown model type: {model_type}")
        
        if attention_weights.numel() == 0:
            print(f"    âš  Warning: Empty attention weights for {model_type}")
            patch_size = 16 if model_type in ["clip", "univfd", "drct"] else (7 if model_type == "drct_convnext" else 14)
            return np.zeros((patch_size, patch_size))
        
        batch_size, num_patches = attention_weights.shape
        patch_size = int(np.sqrt(num_patches))
        attention_map = attention_weights.reshape(patch_size, patch_size)
        
        att_min = attention_map.min()
        att_max = attention_map.max()
        
        if att_max == att_min:
            print(f"    âš  Warning: Uniform attention map for {model_type}, using 0.5")
            attention_map = torch.ones_like(attention_map) * 0.5
        else:
            attention_map = (attention_map - att_min) / (att_max - att_min + 1e-8)
        
        return attention_map.cpu().numpy()


def compute_multi_scale_attention(model, image_tensor, model_type):
    """
    å¤šå°ºåº¦æ³¨æ„åŠ›ï¼šç»“åˆç‰¹å¾èŒƒæ•°å’Œç›¸ä¼¼åº¦
    """
    if image_tensor is None or image_tensor.numel() == 0:
        print(f"    âš  Warning: Empty input tensor for {model_type}")
        patch_size = 16 if model_type in ["clip", "univfd", "drct"] else (7 if model_type == "drct_convnext" else 14)
        return np.zeros((patch_size, patch_size))
    
    # ğŸ”¥ å¯¹äº drct_convnextï¼Œä½¿ç”¨ feature_norm
    if model_type in ["clip", "univfd", "drct", "drct_convnext"]:
        print(f"    Using feature_norm for {model_type} attention map (GradCAM disabled).")
        try:
            attention_norm = compute_attention_map(model, image_tensor, model_type, method='feature_norm')
            
            if attention_norm is None or (isinstance(attention_norm, np.ndarray) and attention_norm.size == 0):
                print(f"    âš  Warning: Empty attention map from feature_norm")
                patch_size = 16 if model_type in ["clip", "univfd", "drct"] else 7
                return np.zeros((patch_size, patch_size))
            
            attention_sim = get_attention_map_alternative(model, image_tensor, model_type)
            
            if attention_sim is None or (isinstance(attention_sim, np.ndarray) and attention_sim.size == 0):
                attention_sim = attention_norm
            
            attention_combined = 0.6 * attention_norm + 0.4 * attention_sim
            
            if attention_combined.max() == attention_combined.min():
                attention_combined = np.ones_like(attention_combined) * 0.5
            else:
                attention_combined = (attention_combined - attention_combined.min()) / (attention_combined.max() - attention_combined.min() + 1e-8)
            
            return attention_combined
            
        except Exception as e:
            print(f"    âœ— Error in attention computation for {model_type}: {e}")
            patch_size = 16 if model_type in ["clip", "univfd", "drct"] else 7
            return np.zeros((patch_size, patch_size))
    
    # å¯¹äºå…¶ä»–æ¨¡å‹ï¼Œä¼˜å…ˆå°è¯•GradCAM
    try:
        attention_map_gradcam = get_gradcam_attention_map(model, image_tensor, model_type)
        
        if attention_map_gradcam is not None and attention_map_gradcam.max() > 0:
            print(f"    Using GradCAM for {model_type} attention map.")
            return attention_map_gradcam
    except Exception as e:
        print(f"    âš  GradCAM failed for {model_type}: {e}")

    print(f"    GradCAM failed for {model_type}, falling back to feature_norm + similarity.")
    
    try:
        # æ–¹æ³•1: ç‰¹å¾èŒƒæ•°
        attention_norm = compute_attention_map(model, image_tensor, model_type, method='feature_norm')
        
        if attention_norm is None or (isinstance(attention_norm, np.ndarray) and attention_norm.size == 0):
            print(f"    âš  Warning: Empty attention_norm")
            patch_size = 16 if model_type in ["clip", "effort"] else 14
            return np.zeros((patch_size, patch_size))
        
        # æ–¹æ³•2: ç›¸ä¼¼åº¦
        attention_sim = get_attention_map_alternative(model, image_tensor, model_type)
        
        if attention_sim is None or (isinstance(attention_sim, np.ndarray) and attention_sim.size == 0):
            print(f"    âš  Warning: Empty attention_sim, using attention_norm only")
            attention_sim = attention_norm
        
        # èåˆä¸¤ç§æ–¹æ³•
        attention_combined = 0.6 * attention_norm + 0.4 * attention_sim
        
        # ğŸ”¥ å†æ¬¡å½’ä¸€åŒ–å‰æ£€æŸ¥
        if attention_combined.max() == attention_combined.min():
            attention_combined = np.ones_like(attention_combined) * 0.5
        else:
            attention_combined = (attention_combined - attention_combined.min()) / (attention_combined.max() - attention_combined.min() + 1e-8)
        
        return attention_combined
        
    except Exception as e:
        print(f"    âœ— Error in fallback attention computation for {model_type}: {e}")
        import traceback
        traceback.print_exc()
        patch_size = 16 if model_type in ["clip", "effort"] else 14
        return np.zeros((patch_size, patch_size))





def compute_pca_features(dense_features):
    """è®¡ç®—PCAç‰¹å¾"""
    batch_size, feature_dim, h, w = dense_features.shape
    features_flat = dense_features.squeeze(0).permute(1, 2, 0).reshape(-1, feature_dim).cpu().numpy()
    
    try:
        pca = PCA(n_components=3)
        pca_features = pca.fit_transform(features_flat)
    except:
        feature_variance = np.var(features_flat, axis=0)
        top_3_indices = np.argsort(feature_variance)[-3:]
        pca_features = features_flat[:, top_3_indices]
    
    pca_features = (pca_features - pca_features.min(axis=0)) / (pca_features.max(axis=0) - pca_features.min(axis=0))
    pca_image = (pca_features * 255).astype(np.uint8).reshape(h, w, 3)
    
    return pca_image


# ============================================================================
# Motion Blur Analysis Functions - ä¸»è¦ä¿®æ”¹è¿™é‡Œ
# ============================================================================
def analyze_motion_blur_effects(model, processor, image_files, output_dir, model_name, model_type, clip_preprocess=None):
    """åˆ†æè¿åŠ¨æ¨¡ç³Šå¯¹ç‰¹å¾å’Œæ³¨æ„åŠ›å›¾çš„å½±å“ï¼Œå¹¶æ”¶é›†æ³¨æ„åŠ›ç›¸ä¼¼åº¦æ•°æ®"""
    print(f"{'='*80}")
    print(f"MOTION BLUR ANALYSIS - [{model_name}] Attention Map Similarity Collection")
    print('='*80)
    
    min_strength, max_strength, step = BLUR_STRENGTH_RANGE
    blur_strengths = np.arange(min_strength, max_strength + step, step)
    blur_strengths = [round(s, 3) for s in blur_strengths]
    
    print(f"Blur strengths to test: {blur_strengths}")
    print(f"Processing {len(image_files)} images...")
    print(f"Using attention computation method: multi-scale (GradCAM + feature_norm + similarity)")
    
    # å­˜å‚¨æ‰€æœ‰å›¾ç‰‡çš„ç›¸ä¼¼åº¦æ•°æ®
    # ç»“æ„: {image_name: {blur_strength: similarity, ...}}
    all_image_similarities = {} 
    
    for img_idx, image_path in enumerate(image_files):
        image_name = image_path.stem
        print(f"[{img_idx+1}/{len(image_files)}] Analyzing motion blur effects for: {image_name}")
        
        try:
            original_image = Image.open(image_path).convert('RGB')
            original_size = original_image.size
            
            # è®¡ç®—åŸå§‹å›¾ç‰‡çš„æ³¨æ„åŠ›å›¾
            original_image_tensor, _, _ = preprocess_image_for_blur_analysis(original_image, model_type, processor, clip_preprocess)
            original_attention_map = compute_multi_scale_attention(model, original_image_tensor, model_type)
            
            if original_attention_map.max() == 0:
                print(f"    âš  Warning: Original attention map for {image_name} is all zeros! Skipping this image.")
                continue

            image_similarities = {}
            
            for strength in blur_strengths: # è¿­ä»£æ‰€æœ‰çš„æ¨¡ç³Šå¼ºåº¦
                print(f"  Processing blur strength: {strength}")
                
                if strength == 0.0:
                    test_image = original_image
                    attention_map = original_attention_map
                else:
                    test_image = apply_motion_blur_to_pil_image(original_image, strength, BLUR_ANGLE)
                    image_tensor, _, _ = preprocess_image_for_blur_analysis(test_image, model_type, processor, clip_preprocess)
                    attention_map = compute_multi_scale_attention(model, image_tensor, model_type)
                
                # è®¡ç®—ä¸åŸå§‹æ³¨æ„åŠ›å›¾çš„ç›¸ä¼¼åº¦ (çš®å°”é€Šç›¸å…³ç³»æ•°)
                if attention_map.max() == 0: # å¦‚æœæ¨¡ç³Šåçš„æ³¨æ„åŠ›å›¾ä¹Ÿå…¨ä¸º0
                    similarity = 0.0 # è§†ä¸ºä¸ç›¸ä¼¼
                    print(f"    âš  Warning: Blurred attention map for {image_name} at strength {strength} is all zeros, similarity set to 0.")
                else:
                    correlation = np.corrcoef(original_attention_map.flatten(), attention_map.flatten())[0, 1]
                    similarity = correlation if not np.isnan(correlation) else 0.0
                
                image_similarities[f'{strength:.3f}'] = float(similarity) # ä¿å­˜ä¸ºæµ®ç‚¹æ•°
                print(f"    Similarity to original: {similarity:.4f}")
            
            all_image_similarities[image_name] = image_similarities
            print(f"  âœ“ Completed motion blur analysis for {image_name}")
            
        except Exception as e:
            print(f"  âœ— Error analyzing {image_name}: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # å°†ç»“æœä¿å­˜åˆ°JSONæ–‡ä»¶
    output_filepath = os.path.join(output_dir, MOTION_BLUR_OUTPUT_DIR, f'{model_name}_attention_similarities.json')
    Path(os.path.dirname(output_filepath)).mkdir(parents=True, exist_ok=True)
    with open(output_filepath, 'w') as f:
        json.dump(all_image_similarities, f, indent=4)
    print(f"âœ“ Attention similarity data saved to: {output_filepath}")
    
    print(f"Motion blur analysis completed for model {model_name}!")
    return all_image_similarities # è¿”å›æ”¶é›†åˆ°çš„æ•°æ®



# ============================================================================
# æ–°å¢ï¼šæœ€ç»ˆç»˜å›¾åŠŸèƒ½
# ============================================================================
# ============================================================================
# ä¿®æ”¹: æœ€ç»ˆç»˜å›¾åŠŸèƒ½ - æ”¯æŒå­é›†åç§° (ä¿®æ­£å‡½æ•°ç­¾å)
# ============================================================================
def plot_attention_similarity_curve(all_models_avg_similarities, output_base_dir, 
                                   blur_strengths, subset_name="all"):  # æ·»åŠ  subset_name å‚æ•°
    """
    ç»˜åˆ¶æ³¨æ„åŠ›å›¾ç›¸ä¼¼åº¦æ›²çº¿å›¾ï¼Œç»“åˆfinalä»£ç çš„æ ·å¼
    
    Args:
        all_models_avg_similarities: {model_name: {blur_strength_str: avg_similarity, ...}}
        output_base_dir: è¾“å‡ºç›®å½•
        blur_strengths: æ¨¡ç³Šå¼ºåº¦åˆ—è¡¨
        subset_name: å­é›†åç§° (ç”¨äºæ–‡ä»¶å‘½åå’Œæ ‡é¢˜)
    """
    print(f"  Generating curve for subset: {subset_name}")

    plt.figure(figsize=(8, 6))

    # å®šä¹‰æ¨¡å‹åç§°åˆ°é¢œè‰²å’Œæ ‡è®°çš„æ˜ å°„ (æ›´æ–°æ¨¡å‹åˆ—è¡¨)
    # ğŸ”¥ æ›´æ–°æ¨¡å‹é…ç½®ï¼Œå°†drctæ›¿æ¢ä¸ºdrct_convnext
    model_plot_configs = {
        'student': {'label': 'DINOV3', 'color': COL_DINO, 'marker': 'o'},
        'custom_vit': {'label': 'Community Forensics', 'color': COL_CUSTOM_VIT, 'marker': '^'},
        'clip': {'label': 'CLIP', 'color': COL_CLIP, 'marker': 's'},
        'univfd': {'label': 'UnivFD', 'color': COL_UNIVFD, 'marker': 'D'},
        'drct_convnext': {'label': 'DRCT ConvB', 'color': COL_DRCT_CONVB, 'marker': 'v'},  # ğŸ”¥ ä¿®æ”¹è¿™é‡Œ
        'aide': {'label': 'AIDE', 'color': COL_AIDE, 'marker': 'p'},
    }

    # ç¡®ä¿ blur_strengths æ˜¯æµ®ç‚¹æ•°æ•°ç»„
    plot_blur_strengths = np.array(blur_strengths).astype(float)
    
    for model_name, avg_similarities in all_models_avg_similarities.items():
        if model_name in model_plot_configs:
            config = model_plot_configs[model_name]
            # ç¡®ä¿ avg_similarities ä¹Ÿæ˜¯æµ®ç‚¹æ•°æ•°ç»„ï¼Œå¹¶ä¸”ä¸ blur_strengths é•¿åº¦åŒ¹é…
            plot_similarities = np.array([
                avg_similarities.get(f'{s:.3f}', 0.0) for s in blur_strengths
            ]).astype(float)
            
            plt.plot(plot_blur_strengths, plot_similarities, 
                     marker=config['marker'], linewidth=LINEWIDTH, markersize=MARKERSIZE,
                     label=config['label'], color=config['color'])
        else:
            print(f"    Warning: Model '{model_name}' not configured for plotting, skipping.")

    # æ ¹æ®å­é›†åç§°è°ƒæ•´æ ‡é¢˜
    subset_display_names = {
        "0_real": "REAL Images (0_real)",
        "1_fake": "FAKE Images (1_fake)",
        "all": "All Images"
    }
    title_suffix = subset_display_names.get(subset_name, subset_name)
    
    plt.title(f'Attention Map Similarity Against Motion Blur {title_suffix}', 
              fontsize=16, pad=14)
    plt.xlabel('Motion Blur Strength', fontsize=13, labelpad=8)
    plt.ylabel('Attention Similarity to Original', fontsize=13, labelpad=8)

    plt.ylim(0.0, 1.05)
    plt.xlim(-0.005, 0.205)
    plt.xticks(plot_blur_strengths)
    plt.tick_params(axis='both', which='major', labelsize=11, length=6, width=1)

    plt.grid(True, which='major', color=GRID_COLOR, alpha=GRID_ALPHA,
             linestyle=GRID_LS, linewidth=GRID_LW)

    ax = plt.gca()
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)

    leg = plt.legend(loc='lower left', frameon=True, fancybox=True, framealpha=0.9,
                     fontsize=11, title='Models')
    plt.setp(leg.get_title(), fontsize=11)

    plt.tight_layout()

    # æ ¹æ®å­é›†åç§°ç”Ÿæˆæ–‡ä»¶å
    output_filename = f'attention_similarity_vs_blur_{subset_name}.png'
    output_path = os.path.join(output_base_dir, output_filename)
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"  âœ“ Curve saved to: {output_path}")


# ============================================================================
# Main Function - æ”¯æŒå¤šä¸ªæ¨¡å‹å’Œå¤šä¸ªå­é›†åˆ†æ
# ============================================================================
def main(batch_size=8, use_batch_processing=False, max_workers=4):
    """ä¸»å‡½æ•° - æ”¯æŒå¤šæ¨¡å‹ã€å¤šå­é›†æ‰¹é‡å¤„ç†"""
    print("Dense Feature Visualization - Multi-Model Multi-Subset Analysis")
    print("=" * 80)
    
    # å®šä¹‰è¦æµ‹è¯•çš„æ¨¡å‹ (æ·»åŠ  Effort å’Œ AIDE)
    models_to_test = [
        {
            "name": "drct_convnext",  #  ä¿®æ”¹è¿™é‡Œ
            "display_name": "DRCT ConvB",
            "type": "drct_convnext",
            "path": DRCT_CONVNEXT_CHECKPOINT
        }
    ]
    
    
    # åˆ›å»ºä¸»è¾“å‡ºæ–‡ä»¶å¤¹
    if not os.path.exists(OUTPUT_BASE_DIR):
        os.makedirs(OUTPUT_BASE_DIR)
        print(f"Created main output directory: {OUTPUT_BASE_DIR}")
    
    # åŠ è½½DINOv3ä¸“ç”¨çš„processor
    print("Loading DINOv3 image processor...")
    dinov3_processor = AutoImageProcessor.from_pretrained(BASE_MODEL_PATH)
    print("âœ“ DINOv3 Processor loaded\n")
    
    # æ”¶é›†å›¾ç‰‡æ–‡ä»¶ - è¯»å–æ‰€æœ‰å›¾ç‰‡
    print("=" * 80)
    print("Collecting REAL images (0_real)...")
    print("=" * 80)
    real_image_files = get_image_files_from_directory(REAL_ORIGINAL_DIR, num_images=None)  # Noneè¡¨ç¤ºè¯»å–æ‰€æœ‰
    
    print("" + "=" * 80)
    print("Collecting FAKE images (1_fake)...")
    print("=" * 80)
    fake_image_files = get_image_files_from_directory(FAKE_ORIGINAL_DIR, num_images=None)  # Noneè¡¨ç¤ºè¯»å–æ‰€æœ‰
    
    # åˆå¹¶æ‰€æœ‰å›¾ç‰‡
    all_image_files = real_image_files + fake_image_files
    
    if not all_image_files:
        print("No images found! Exiting.")
        return
    
    total_count = len(all_image_files)
    real_count = len(real_image_files)
    fake_count = len(fake_image_files)
    
    print(f"{'='*80}")
    print(f"Dataset Summary:")
    print(f"  REAL images (0_real): {real_count}")
    print(f"  FAKE images (1_fake): {fake_count}")
    print(f"  Total images: {total_count}")
    print(f"{'='*80}")
    
    # è®¡ç®—blur_strengthsåˆ—è¡¨
    min_strength, max_strength, step = BLUR_STRENGTH_RANGE
    blur_strengths = np.arange(min_strength, max_strength + step, step)
    blur_strengths = [round(s, 3) for s in blur_strengths]
    
    # å®šä¹‰è¦åˆ†æçš„å­é›†
    subsets_to_analyze = [
        {
            "name": "all",
            "display_name": "All Images (0_real + 1_fake)",
            "image_files": all_image_files
        }
    ]
    
    # å­˜å‚¨æ‰€æœ‰æ¨¡å‹ã€æ‰€æœ‰å­é›†çš„å¹³å‡ç›¸ä¼¼åº¦æ•°æ®
    # ç»“æ„: {subset_name: {model_name: {blur_strength_str: avg_similarity, ...}}}
    all_subsets_all_models_data = {}
    
    # å¤„ç†æ¯ä¸ªæ¨¡å‹
    for model_config in models_to_test:
        model_name = model_config["name"]
        model_type = model_config["type"]
        
        print(f"{'#'*100}")
        print(f"# Processing Model: {model_config['display_name']} (Type: {model_type})")
        print(f"{'#'*100}")
        
        # åˆ›å»ºæ¨¡å‹ä¸“å±è¾“å‡ºæ–‡ä»¶å¤¹
        model_output_dir = os.path.join(OUTPUT_BASE_DIR, model_name)
        if not os.path.exists(model_output_dir):
            os.makedirs(model_output_dir)
            print(f"Created model output directory: {model_output_dir}")
        
        current_model = None
        current_processor = None
        current_clip_preprocess = None
    
        try:
            if model_type == "dinov3_checkpoint":
                current_model = load_model_from_checkpoint(model_config["path"], BASE_MODEL_PATH)
                current_processor = dinov3_processor
                model_type = "dinov3"
                if current_model is None:
                    print(f"âœ— Failed to load DINOv3 model '{model_config['display_name']}', skipping...")
                    continue
                    
            elif model_type == "custom":
                current_model, _ = load_custom_vit_model()
                if current_model is None:
                    print(f"âœ— Failed to load Custom ViT model '{model_config['display_name']}', skipping...")
                    continue
                    
            elif model_type == "clip":
                current_model, current_clip_preprocess = load_clip_model()
                if current_model is None:
                    print(f"âœ— Failed to load CLIP model '{model_config['display_name']}', skipping...")
                    continue
                    
            elif model_type == "univfd":  # æ–°å¢
                current_model, current_clip_preprocess = load_univfd_model()
                if current_model is None:
                    print(f"âœ— Failed to load UnivFD model '{model_config['display_name']}', skipping...")
                    continue
                    
            elif model_type == "drct_convnext":  # ğŸ”¥ æ·»åŠ è¿™ä¸ªåˆ†æ”¯
                current_model, _ = load_drct_convnext_model()
                if current_model is None:
                    print(f"âœ— Failed to load DRCT ConvNeXt model '{model_config['display_name']}', skipping...")
                    continue
                    
            elif model_type == "aide":
                current_model, _ = load_aide_model()
                model_type = "custom"
                if current_model is None:
                    print(f"âœ— Failed to load AIDE model '{model_config['display_name']}', skipping...")
                    continue
                    
            
            else:
                print(f"âœ— Unknown model type '{model_type}', skipping...")
                continue

            current_model.eval()
            
            # ç¡®è®¤æ¨¡å‹åœ¨GPUä¸Š
            if next(current_model.parameters()).is_cuda:
                print(f"âœ“ Model confirmed on GPU: {DEVICE}")
            else:
                print(f"âš  Warning: Model is on CPU, moving to {DEVICE}...")
                current_model = current_model.to(DEVICE)
            
        except Exception as e:
            print(f"âœ— Error loading model '{model_config['display_name']}': {e}")
            import traceback
            traceback.print_exc()
            continue
        
        # å¯¹æ¯ä¸ªå­é›†è¿›è¡Œåˆ†æ
        for subset_config in subsets_to_analyze:
            subset_name = subset_config["name"]
            subset_display_name = subset_config["display_name"]
            subset_image_files = subset_config["image_files"]
            
            if len(subset_image_files) == 0:
                print(f"\nâš  Warning: No images in subset '{subset_name}', skipping...")
                continue
            
            print(f"{'='*80}")
            print(f"Analyzing subset: {subset_display_name}")
            print(f"Model: {model_config['display_name']}")
            print('='*80)
            
            # è°ƒç”¨å­é›†åˆ†æå‡½æ•°
            subset_avg_similarities, all_image_similarities = analyze_subset(
                current_model, current_processor, subset_image_files, 
                model_name, model_type, current_clip_preprocess, 
                blur_strengths, subset_display_name
            )
            
            # ä¿å­˜è¯¥æ¨¡å‹ã€è¯¥å­é›†çš„è¯¦ç»†æ•°æ®åˆ°JSON
            subset_output_dir = os.path.join(model_output_dir, MOTION_BLUR_OUTPUT_DIR, subset_name)
            Path(subset_output_dir).mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜æ¯å¼ å›¾ç‰‡çš„ç›¸ä¼¼åº¦æ•°æ®
            detailed_filepath = os.path.join(
                subset_output_dir, 
                f'{model_name}_{subset_name}_detailed_similarities.json'
            )
            with open(detailed_filepath, 'w') as f:
                json.dump(all_image_similarities, f, indent=4)
            print(f"  âœ“ Detailed similarity data saved to: {detailed_filepath}")
            
            # ä¿å­˜å¹³å‡ç›¸ä¼¼åº¦æ•°æ®
            avg_filepath = os.path.join(
                subset_output_dir, 
                f'{model_name}_{subset_name}_avg_similarities.json'
            )
            with open(avg_filepath, 'w') as f:
                json.dump(subset_avg_similarities, f, indent=4)
            print(f"  âœ“ Average similarity data saved to: {avg_filepath}")
            
            # å­˜å‚¨åˆ°æ€»æ•°æ®ç»“æ„ä¸­
            if subset_name not in all_subsets_all_models_data:
                all_subsets_all_models_data[subset_name] = {}
            all_subsets_all_models_data[subset_name][model_name] = subset_avg_similarities
        
        # æ¸…ç†å†…å­˜
        del current_model
        if current_clip_preprocess is not None:
            del current_clip_preprocess
        torch.cuda.empty_cache()
        print(f"âœ“ Model '{model_config['display_name']}' cleaned from memory")
    
    print(f"{'='*80}")
    print("All models and subsets processed. Generating final curves...")
    print('='*80)
    
    final_output_dir = os.path.join(OUTPUT_BASE_DIR, MOTION_BLUR_OUTPUT_DIR)
    Path(final_output_dir).mkdir(parents=True, exist_ok=True)
    
    for subset_name, models_data in all_subsets_all_models_data.items():
        if models_data:
            print(f"Generating curve for subset: {subset_name}")
            
            # ä¿å­˜è¯¥å­é›†æ‰€æœ‰æ¨¡å‹çš„æ±‡æ€»æ•°æ®
            subset_summary_filepath = os.path.join(
                final_output_dir, 
                f'{subset_name}_all_models_avg_similarities.json'
            )
            with open(subset_summary_filepath, 'w') as f:
                json.dump(models_data, f, indent=4)
            print(f"  âœ“ Subset summary saved to: {subset_summary_filepath}")
            
            # ç»˜åˆ¶è¯¥å­é›†çš„æŠ˜çº¿å›¾
            plot_attention_similarity_curve(
                models_data, 
                final_output_dir, 
                blur_strengths, 
                subset_name=subset_name  # ä¼ å…¥å­é›†åç§°ç”¨äºæ–‡ä»¶å‘½å
            )
    
    # æœ€ç»ˆæ€»ç»“
    print(f"{'#'*100}")
    print("ALL PROCESSING COMPLETED!")
    print(f"{'#'*100}")
    print(f"Total models tested: {len(models_to_test)}")
    print(f"Total subsets analyzed: {len(subsets_to_analyze)}")
    print(f"Dataset summary:")
    print(f"  REAL images (0_real): {real_count}")
    print(f"  FAKE images (1_fake): {fake_count}")
    print(f"  Total images: {total_count}")
    print(f"Main output directory: {OUTPUT_BASE_DIR}")
    print(f"Blur intensity range: {BLUR_STRENGTH_RANGE}")
    print(f"Final curves and data saved in: {final_output_dir}")
    print(f"{'#'*100}")

if __name__ == "__main__":

    USE_BATCH_PROCESSING = False  # å…³é—­å¸¸è§„æ‰¹é‡å¯è§†åŒ–
    
    main(batch_size=BATCH_SIZE, use_batch_processing=USE_BATCH_PROCESSING, max_workers=MAX_WORKERS)
